{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week1. Mastering Text Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자연어처리(Natural Language Processing: NLP)에서, 초기이자 중요한 단계는 텍스트 전처리\n",
    "- 원시 형태의 텍스트 데이터는 기계 학습 알고리즘으로 이해되지 않음\n",
    "- NLP의 힘을 활용하고 효과적인 모델을 개발하려면, 먼저 텍스트 데이터를 구조화된 숫자 형식으로 변환해여 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- 토큰화(Tokenization)은 NLP의 초기 단계로, 더 큰 텍스트를 토큰이라고 하는 더 작은 단위로 나누는 작업을 포함함\n",
    "- 토큰은 텍스트 처리를 위한 기본 구성 요소 역할을 하며, 각 토큰은 더 큰 문서의 컨텍스트 내에서 단어, 구문 또는 전체 문장을 나타낼 수 있음\n",
    "- NLP에서 감정 분석, 품질 보증 시스템, 언어 번역, 지능형 챗봇 및 음성 인식 시스템과 같은 애플리케이션을 개발하려면 텍스트 내의 이러한 패턴을 이해하는 것이 중요함\n",
    "- 토큰은 이러한 텍스트 패턴을 식별하고 이해하는 데 중추적인 역할을 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: \n",
      "['Tokenization', 'is', 'the', 'initial', 'step', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'involving', 'the', 'division', 'of', 'a', 'larger', 'text', 'into', 'smaller', 'units', ',', 'which', 'are', 'known', 'as', 'tokens', '.', 'Tokens', 'serve', 'as', 'the', 'fundamental', 'building', 'blocks', 'for', 'processing', 'text', ',', 'where', 'each', 'token', 'can', 'represent', 'a', 'word', ',', 'phrase', ',', 'or', 'even', 'an', 'entire', 'sentence', 'within', 'the', 'context', 'of', 'a', 'larger', 'document', '.']\n",
      "\n",
      "\n",
      "Tokenized Sentences: \n",
      "['Tokenization is the initial step in Natural Language Processing (NLP), involving the division of a larger text into smaller units, \\nwhich are known as tokens.', 'Tokens serve as the fundamental building blocks for processing text, where each token can represent a word, phrase, \\nor even an entire sentence within the context of a larger document.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"\"\"Tokenization is the initial step in Natural Language Processing (NLP), involving the division of a larger text into smaller units, \n",
    "which are known as tokens. Tokens serve as the fundamental building blocks for processing text, where each token can represent a word, phrase, \n",
    "or even an entire sentence within the context of a larger document.\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(f\"Tokenized Words: \\n{words}\")\n",
    "\n",
    "# print the tokenized sentences\n",
    "print(f\"\\n\\nTokenized Sentences: \\n{sentences}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming(어간 추출)\n",
    "- 어간 추출은 접사를 제거하여 단어를 기본 형태로 되돌려 단어를 단순화하는 것을 목표로 하는 기술\n",
    "  - 예를 들어, \"eating\", \"eats\", \"eaten\" 단어들을 어간 추출에 적용시키면 우리는 기본 단어인 \"eat\"을 얻게 됨\n",
    "- 검색 엔진은 단어를 색인화할 때 어간 추출을 활용하는 경우가 많음\n",
    "- 검색 엔진은 단어의 모든 변형을 저장하는 대신 단어 어간만 효율적으로 저장할 수 있음\n",
    "- 이 접근 방식은 인덱스 크기를 줄이고 검색 정확도를 향상시켜 어간 추출을 정보 검색 시스템의 필수 도구로 만듦"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/stem_module.png\" witdth=800 height=300></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLTK에는 많은 어간 추출 방법이 있음\n",
    "  - 영어에는 4개의 주요 어간 추출이 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer(```nltk.stem.PorterStemmer```)\n",
    "- Porter stemmer는 가장 많이 사용되는 어간 추출 중 하나\n",
    "- 단어를 어간으로 줄이기 위해 일련의 규칙을 적용\n",
    "- 간단하고 빠르지만 항상 사전적 단어를 생성하는 것은 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'happili', 'better', 'jump', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\", \"jumps\", \"happiest\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer(```nltk.stem.LancasterStemmer```)\n",
    "- Lancaster stemmer는 Porter stemmer보다 더 공격적이며 종종 더 짧은 어간 추출을 생성함\n",
    "- 특정 응용 프로그램에는 유용할 수 있지만 덜 직관적인 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'happy', 'bet', 'jump', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer  \n",
    "\n",
    "stemmer = LancasterStemmer() \n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\", \"jumps\", \"happiest\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball Stemmer(```nltk.stem.SnowballStemmer```)\n",
    "- Porter2 stemmer라고 알려진 Snowball stemmer는 더 현대적이고 Porter stemmer 보다 향상된 버전임\n",
    "- 다양한 언어를 지원하며, 언어별 어간 추출과 함께 사용될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'happliy', 'better', 'jump', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "words = [\"running\", \"flies\", \"happliy\", \"better\", \"jumps\", \"happiest\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegExp Stemmer(```nltk.stem.RegexpStemmer```)\n",
    "- 이 어간 추출을 사용하면 정규식을 사용하여 자신만의 어간 추출 규칙을 정의할 수 있음\n",
    "- 특정 도메인이나 언어에 맞게 사용자 정의할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runn', 'fli', 'happi', 'bett', 'jumps', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "stemmer = RegexpStemmer('ing$|es$|ly$|er$', min=4)\n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\", \"jumps\", \"happiest\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization(표제어 추출)\n",
    "- 어간 추출과 같이 마찬가지로 표제어 추출은 단어를 단순화하는 것을 목표로 함\n",
    "- 어간 추출과 달리, 표제어 추출은 표제어 추출은 단순한 어간이 아니라 진정한 기본 단어인 표제어(lemma)를 생성함\n",
    "- 표제어 추출을 통해 원래 단어와 동일한 의미를 전달하는 유요한 용어를 얻음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet Lemmatizer(```nltk.stem.WordNetLemmatizer```)\n",
    "- WordNet Lemmatizer는 표제어를 찾기 위해 영어 어휘 데이터베이스인 WordNet을 사용\n",
    "- 보다 정확한 기본형을 제공하기 위해 단어의 품사(Part of Speech:POS)를 고려함\n",
    "- NLTK에서 가장 일반적으로 사용되는 표제어 추출기 중 하나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'fly', 'happily', 'better', 'jump', 'happiest']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\", \"jumps\", \"happiest\"]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Lemmatizer(```spacy.lemmatizer```)\n",
    "- Spacy는 표제어 추출 기능을 제공하는 또 다른 인기 있는 자연어 처리 라이브러리임\n",
    "- Spacy lemmatizer는 정확도가 높고 컨텍스트를 고려함\n",
    "- Spacy lemmatizer를 사용하기 위해, Spacy 라이브러리를 설치해야하고, 적절한 언어 모델을 다운로드 받아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fly', 'happily', 'well', 'jump', 'happy']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\", \"jumps\", \"happiest\"]\n",
    "\n",
    "lemmatized_words = [token.lemma_ for token in nlp(\" \".join(words))]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Lemmatizer(```textblob.word```)\n",
    "- TextBlob은 NLTK위에 구축된 단순화된 라이브러리임\n",
    "- 표제어 추출을 포함한 텍스트 처리 작업을 위한 간단한 인터페이스를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'fly', 'happily', 'better', 'jump', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "words = [\"running\", \"flies\", \"happily\", \"better\", \"jumps\", \"happiest\"]\n",
    "\n",
    "lemmatized_words = [Word(word).lemmatize() for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Lemmatization and Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/stemming_lemmatization.png\" width=700 height=300></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어간 추출과 표제어 추출은 둘 다 단어를 기본 형태로 줄이는 것을 목표로 하지만 접근 방식과 결과가 다름\n",
    "- 어간 추출은 단어 끝을 다듬어 기본 형태에 근접하게 하고 종종 단순성을 위해 파생 접사를 제거하는 경험적 방법\n",
    "- 표제어 추출은 단어의 의미와 언어적 맥락을 고려하고 굴절 어미만 보존하여 단어의 실제 기본 똔느 사전 형태를 찾으려고 함\n",
    "  - 이 기본 형식을 표제어(lemma)라고 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "- Part of Speech(POS) 태깅은 문장의 단어에 해당 직업이나 역할에 따라 레이블을 지정하는 것과 같음\n",
    "- 명사, 동사, 형용사 등이 있음\n",
    "- 우리가 읽을 때 처럼 알고리즘이 구조와 문장의 의미를 이해하는 것을 도와줌\n",
    "- POS 태깅은 문장 내 기능에 따라 단어를 다양한 그룹으로 분류하여 알고리즘이 텍스트를 처리하고 이해하는 데 도움이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "None\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "None\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "None\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "print(words)\n",
    "\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "print(pos_tags)\n",
    "\n",
    "nltk.download(\"tagsets\")\n",
    "\n",
    "for (word, tag) in pos_tags:\n",
    "    print(nltk.help.upenn_tagset(tagpattern=tag))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "- Chunking은 문장에서 함께 작동하는 단어 그룸을 찾는 것과 같음\n",
    "- 구문을 식별하거나 의미의 구성 요소를 구성하는 것과 같음\n",
    "- 각 단어를 개별적으로 보는 대신 함께 속한 단어를 그룹화하여 더 큰 그림을 볼 수 있도록 도와줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/leesongjae/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,328.0,168.0\" width=\"328px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"53.6585%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"22.7273%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"22.7273%\" x=\"22.7273%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">big</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.0909%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"31.8182%\" x=\"45.4545%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">brown</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.3636%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"22.7273%\" x=\"77.2727%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.6364%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.8293%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"19.5122%\" x=\"53.6585%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">barked</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.4146%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"19.5122%\" x=\"73.1707%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">loudly</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.9268%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.31707%\" x=\"92.6829%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.3415%\" y1=\"19.2px\" y2=\"48px\" /></svg>",
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('brown', 'NN'), ('dog', 'NN')]), ('barked', 'VBD'), ('loudly', 'RB'), ('.', '.')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "\n",
    "sentence = \"The big brown dog barked loudly.\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "chunked = nltk.ne_chunk(pos_tags)\n",
    "\n",
    "reg_exp = \"NP: {<DT>?<JJ>*<NN>*}\"\n",
    "rp = nltk.RegexpParser(reg_exp)\n",
    "result = rp.parse(chunked)\n",
    "\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOB Tagging\n",
    "- Inside, Outside, Beginning의 약어인 IOB 태깅은 특히 사람 이름이나 지역과 같은 명명된 엔티티를 처리할 때 문장의 단어를 레이블 및 그룹화하는 데 사용되는 방법임\n",
    "- Inside(I): New York의 New와 같이 명명된 엔티티의 일부인 단어를 찾으면 엔티티 내부에 있음을 표시하기 위해 해당 단어에 I라는 레이블을 붙임\n",
    "- Outside(O): 명명된 엔티티의 일부가 아닌 단어는 O로 표시되어 엔티티 외부에 있음을 나타냄\n",
    "- Beginning(B): 명명된 엔티티의 첫 번째 단어는 B로 레이블이 지정되어 해당 단어가 엔티티의 시작임을 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d517b10c2f7709916c423353ce86a4ea5fd8e19f64ae4345560ba4c7f9e837d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
